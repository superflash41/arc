{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e32706f4",
   "metadata": {},
   "source": [
    "# ARC AGI Benchmark Task Solutions Testing\n",
    "\n",
    "This notebook tests the solve functions against the ARC AGI benchmark tasks from the test_set folder.\n",
    "\n",
    "We'll test 5 different tasks:\n",
    "- 05f2a901\n",
    "- 1cf80156\n",
    "- 1e0a9b12\n",
    "- 2bcee788\n",
    "- 7ddcd7ec\n",
    "\n",
    "For each task, we'll:\n",
    "1. Load the task data (train and test examples)\n",
    "2. Apply the solve function\n",
    "3. Compare with expected outputs\n",
    "4. Calculate accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34eb030a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from typing import Tuple\n",
    "\n",
    "# add dsl folder to path\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), 'dsl'))\n",
    "\n",
    "# import all DSL functions\n",
    "from dsl import *\n",
    "from constants import *\n",
    "from arc_types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab9b4ba",
   "metadata": {},
   "source": [
    "## Define Solve Functions\n",
    "\n",
    "These are the solve functions that combine atomic transformations to solve specific ARC tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f8a1bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_1cf80156(I):\n",
    "    x1 = objects(I, T, T, T)\n",
    "    x2 = first(x1)\n",
    "    O = subgrid(x2, I)\n",
    "    return O\n",
    "\n",
    "\n",
    "def solve_1e0a9b12(I):\n",
    "    x1 = rot270(I)\n",
    "    x2 = rbind(order, identity)\n",
    "    x3 = apply(x2, x1)\n",
    "    O = rot90(x3)\n",
    "    return O\n",
    "\n",
    "\n",
    "def solve_2bcee788(I):\n",
    "    x1 = mostcolor(I)\n",
    "    x2 = objects(I, T, F, T)\n",
    "    x3 = replace(I, x1, THREE)\n",
    "    x4 = argmax(x2, size)\n",
    "    x5 = argmin(x2, size)\n",
    "    x6 = position(x4, x5)\n",
    "    x7 = first(x6)\n",
    "    x8 = last(x6)\n",
    "    x9 = subgrid(x4, x3)\n",
    "    x10 = hline(x5)\n",
    "    x11 = hmirror(x9)\n",
    "    x12 = vmirror(x9)\n",
    "    x13 = branch(x10, x11, x12)\n",
    "    x14 = branch(x10, x7, ZERO)\n",
    "    x15 = branch(x10, ZERO, x8)\n",
    "    x16 = asobject(x13)\n",
    "    x17 = matcher(first, THREE)\n",
    "    x18 = compose(flip, x17)\n",
    "    x19 = sfilter(x16, x18)\n",
    "    x20 = ulcorner(x4)\n",
    "    x21 = shape(x4)\n",
    "    x22 = astuple(x14, x15)\n",
    "    x23 = multiply(x21, x22)\n",
    "    x24 = add(x20, x23)\n",
    "    x25 = shift(x19, x24)\n",
    "    O = paint(x3, x25)\n",
    "    return O\n",
    "\n",
    "\n",
    "def solve_05f2a901(I):\n",
    "    x1 = objects(I, T, F, T)\n",
    "    x2 = colorfilter(x1, TWO)\n",
    "    x3 = first(x2)\n",
    "    x4 = colorfilter(x1, EIGHT)\n",
    "    x5 = first(x4)\n",
    "    x6 = gravitate(x3, x5)\n",
    "    O = move(I, x3, x6)\n",
    "    return O\n",
    "\n",
    "\n",
    "def solve_7ddcd7ec(I):\n",
    "    x1 = objects(I, T, F, T)\n",
    "    x2 = sizefilter(x1, ONE)\n",
    "    x3 = difference(x1, x2)\n",
    "    x4 = first(x3)\n",
    "    x5 = color(x4)\n",
    "    x6 = lbind(position, x4)\n",
    "    x7 = fork(shoot, center, x6)\n",
    "    x8 = mapply(x7, x2)\n",
    "    O = fill(I, x5, x8)\n",
    "    return O"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6912c6ef",
   "metadata": {},
   "source": [
    "## Utility Functions\n",
    "\n",
    "Helper functions to load task data and compare results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed88affe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_task(task_id: str) -> dict:\n",
    "    \"\"\"load a task from the test_set folder.\"\"\"\n",
    "    task_path = os.path.join('dsl', 'test_set', f'{task_id}.json')\n",
    "    with open(task_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "def grid_to_tuple(grid: list) -> Grid:\n",
    "    \"\"\"convert a list grid to a tuple grid for DSL functions.\"\"\"\n",
    "    return tuple(tuple(row) for row in grid)\n",
    "\n",
    "\n",
    "def tuple_to_list(grid: Grid) -> list:\n",
    "    \"\"\"convert a tuple grid back to a list for comparison.\"\"\"\n",
    "    if isinstance(grid, tuple):\n",
    "        return [list(row) if isinstance(row, tuple) else row for row in grid]\n",
    "    return grid\n",
    "\n",
    "\n",
    "def compare_grids(output: Grid, expected: list) -> Tuple[bool, float]:\n",
    "    \"\"\"\n",
    "    compare output grid with expected output.\n",
    "    returns (is_exact_match, pixel_accuracy)\n",
    "    \"\"\"\n",
    "    output_list = tuple_to_list(output)\n",
    "    \n",
    "    # Check if shapes match\n",
    "    if len(output_list) != len(expected):\n",
    "        return False, 0.0\n",
    "    if len(output_list) > 0 and len(output_list[0]) != len(expected[0]):\n",
    "        return False, 0.0\n",
    "    \n",
    "    # Calculate pixel accuracy\n",
    "    total_pixels = len(expected) * len(expected[0]) if len(expected) > 0 else 0\n",
    "    if total_pixels == 0:\n",
    "        return True, 1.0\n",
    "    \n",
    "    correct_pixels = 0\n",
    "    for i in range(len(expected)):\n",
    "        for j in range(len(expected[0])):\n",
    "            if output_list[i][j] == expected[i][j]:\n",
    "                correct_pixels += 1\n",
    "    \n",
    "    accuracy = correct_pixels / total_pixels\n",
    "    is_exact = accuracy == 1.0\n",
    "    \n",
    "    return is_exact, accuracy\n",
    "\n",
    "\n",
    "def test_task(task_id: str, solve_func):\n",
    "    \"\"\"\n",
    "    Test a solve function on a specific task.\n",
    "    Returns results dict with training and test performance.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Testing Task: {task_id}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    task_data = load_task(task_id)\n",
    "    results = {\n",
    "        'task_id': task_id,\n",
    "        'train_examples': [],\n",
    "        'test_examples': [],\n",
    "        'train_accuracy': 0.0,\n",
    "        'test_accuracy': 0.0,\n",
    "        'train_exact_matches': 0,\n",
    "        'test_exact_matches': 0\n",
    "    }\n",
    "    \n",
    "    # Test on training examples\n",
    "    print(f\"\\nTraining Examples ({len(task_data['train'])} examples):\")\n",
    "    for idx, example in enumerate(task_data['train']):\n",
    "        input_grid = grid_to_tuple(example['input'])\n",
    "        expected_output = example['output']\n",
    "        \n",
    "        try:\n",
    "            output = solve_func(input_grid)\n",
    "            is_exact, accuracy = compare_grids(output, expected_output)\n",
    "            \n",
    "            results['train_examples'].append({\n",
    "                'example_idx': idx,\n",
    "                'is_exact': is_exact,\n",
    "                'accuracy': accuracy\n",
    "            })\n",
    "            \n",
    "            status = \"PASS\" if is_exact else f\"FAIL ({accuracy*100:.1f}%)\"\n",
    "            print(f\"  Example {idx+1}: {status}\")\n",
    "            \n",
    "            if is_exact:\n",
    "                results['train_exact_matches'] += 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  Example {idx+1}: ERROR - {str(e)}\")\n",
    "            results['train_examples'].append({\n",
    "                'example_idx': idx,\n",
    "                'is_exact': False,\n",
    "                'accuracy': 0.0,\n",
    "                'error': str(e)\n",
    "            })\n",
    "    \n",
    "    # calculate training accuracy\n",
    "    if results['train_examples']:\n",
    "        results['train_accuracy'] = np.mean([ex['accuracy'] for ex in results['train_examples']])\n",
    "    \n",
    "    # test on test examples\n",
    "    print(f\"\\nTest Examples ({len(task_data['test'])} examples):\")\n",
    "    for idx, example in enumerate(task_data['test']):\n",
    "        input_grid = grid_to_tuple(example['input'])\n",
    "        expected_output = example['output']\n",
    "        \n",
    "        try:\n",
    "            output = solve_func(input_grid)\n",
    "            is_exact, accuracy = compare_grids(output, expected_output)\n",
    "            \n",
    "            results['test_examples'].append({\n",
    "                'example_idx': idx,\n",
    "                'is_exact': is_exact,\n",
    "                'accuracy': accuracy\n",
    "            })\n",
    "            \n",
    "            status = \"PASS\" if is_exact else f\"FAIL ({accuracy*100:.1f}%)\"\n",
    "            print(f\"  Example {idx+1}: {status}\")\n",
    "            \n",
    "            if is_exact:\n",
    "                results['test_exact_matches'] += 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  Example {idx+1}: ERROR - {str(e)}\")\n",
    "            results['test_examples'].append({\n",
    "                'example_idx': idx,\n",
    "                'is_exact': False,\n",
    "                'accuracy': 0.0,\n",
    "                'error': str(e)\n",
    "            })\n",
    "    \n",
    "    # Calculate test accuracy\n",
    "    if results['test_examples']:\n",
    "        results['test_accuracy'] = np.mean([ex['accuracy'] for ex in results['test_examples']])\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n Task Summary:\")\n",
    "    print(f\"  Training: {results['train_exact_matches']}/{len(task_data['train'])} exact matches, \"\n",
    "          f\"avg accuracy: {results['train_accuracy']*100:.1f}%\")\n",
    "    print(f\"  Test: {results['test_exact_matches']}/{len(task_data['test'])} exact matches, \"\n",
    "          f\"avg accuracy: {results['test_accuracy']*100:.1f}%\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42be3c1",
   "metadata": {},
   "source": [
    "## Run Tests on All Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db3b8be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Testing Task: 05f2a901\n",
      "============================================================\n",
      "\n",
      "Training Examples (3 examples):\n",
      "  Example 1: PASS\n",
      "  Example 2: PASS\n",
      "  Example 3: PASS\n",
      "\n",
      "Test Examples (1 examples):\n",
      "  Example 1: PASS\n",
      "\n",
      " Task Summary:\n",
      "  Training: 3/3 exact matches, avg accuracy: 100.0%\n",
      "  Test: 1/1 exact matches, avg accuracy: 100.0%\n",
      "\n",
      "============================================================\n",
      "Testing Task: 1cf80156\n",
      "============================================================\n",
      "\n",
      "Training Examples (3 examples):\n",
      "  Example 1: PASS\n",
      "  Example 2: PASS\n",
      "  Example 3: PASS\n",
      "\n",
      "Test Examples (1 examples):\n",
      "  Example 1: PASS\n",
      "\n",
      " Task Summary:\n",
      "  Training: 3/3 exact matches, avg accuracy: 100.0%\n",
      "  Test: 1/1 exact matches, avg accuracy: 100.0%\n",
      "\n",
      "============================================================\n",
      "Testing Task: 1e0a9b12\n",
      "============================================================\n",
      "\n",
      "Training Examples (3 examples):\n",
      "  Example 1: PASS\n",
      "  Example 2: PASS\n",
      "  Example 3: PASS\n",
      "\n",
      "Test Examples (1 examples):\n",
      "  Example 1: PASS\n",
      "\n",
      " Task Summary:\n",
      "  Training: 3/3 exact matches, avg accuracy: 100.0%\n",
      "  Test: 1/1 exact matches, avg accuracy: 100.0%\n",
      "\n",
      "============================================================\n",
      "Testing Task: 2bcee788\n",
      "============================================================\n",
      "\n",
      "Training Examples (4 examples):\n",
      "  Example 1: PASS\n",
      "  Example 2: PASS\n",
      "  Example 3: PASS\n",
      "  Example 4: PASS\n",
      "\n",
      "Test Examples (1 examples):\n",
      "  Example 1: PASS\n",
      "\n",
      " Task Summary:\n",
      "  Training: 4/4 exact matches, avg accuracy: 100.0%\n",
      "  Test: 1/1 exact matches, avg accuracy: 100.0%\n",
      "\n",
      "============================================================\n",
      "Testing Task: 7ddcd7ec\n",
      "============================================================\n",
      "\n",
      "Training Examples (3 examples):\n",
      "  Example 1: PASS\n",
      "  Example 2: PASS\n",
      "  Example 3: PASS\n",
      "\n",
      "Test Examples (1 examples):\n",
      "  Example 1: PASS\n",
      "\n",
      " Task Summary:\n",
      "  Training: 3/3 exact matches, avg accuracy: 100.0%\n",
      "  Test: 1/1 exact matches, avg accuracy: 100.0%\n",
      "\n",
      "============================================================\n",
      "Testing Complete!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# define the tasks and their corresponding solve functions\n",
    "tasks = [\n",
    "    ('05f2a901', solve_05f2a901),\n",
    "    ('1cf80156', solve_1cf80156),\n",
    "    ('1e0a9b12', solve_1e0a9b12),\n",
    "    ('2bcee788', solve_2bcee788),\n",
    "    ('7ddcd7ec', solve_7ddcd7ec)\n",
    "]\n",
    "\n",
    "# run tests on all tasks\n",
    "all_results = []\n",
    "\n",
    "for task_id, solve_func in tasks:\n",
    "    try:\n",
    "        results = test_task(task_id, solve_func)\n",
    "        all_results.append(results)\n",
    "    except Exception as e:\n",
    "        print(f\"\\n Failed to test task {task_id}: {str(e)}\")\n",
    "        all_results.append({\n",
    "            'task_id': task_id,\n",
    "            'error': str(e),\n",
    "            'train_accuracy': 0.0,\n",
    "            'test_accuracy': 0.0\n",
    "        })\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Testing Complete!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3a5abc",
   "metadata": {},
   "source": [
    "## Overall Performance Summary\n",
    "\n",
    "Final scores across all tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8161fdb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "OVERALL PERFORMANCE SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Task ID         Train Acc       Test Acc        Train Match     Test Match     \n",
      "----------------------------------------------------------------------\n",
      "05f2a901         100.0%          100.0%         3/3           1/1          \n",
      "1cf80156         100.0%          100.0%         3/3           1/1          \n",
      "1e0a9b12         100.0%          100.0%         3/3           1/1          \n",
      "2bcee788         100.0%          100.0%         4/4           1/1          \n",
      "7ddcd7ec         100.0%          100.0%         3/3           1/1          \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "OVERALL          100.0%          100.0%         16/16           5/5          \n",
      "\n",
      "> Summary:\n",
      "   • Average Training Accuracy: 100.0%\n",
      "   • Average Test Accuracy: 100.0%\n",
      "   • Total Training Exact Matches: 16/16 (100.0%)\n",
      "   • Total Test Exact Matches: 5/5 (100.0%)\n",
      "   • Tasks Tested: 5\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"OVERALL PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Calculate overall statistics\n",
    "total_train_accuracy = []\n",
    "total_test_accuracy = []\n",
    "total_train_exact = 0\n",
    "total_test_exact = 0\n",
    "total_train_examples = 0\n",
    "total_test_examples = 0\n",
    "\n",
    "print(f\"\\n{'Task ID':<15} {'Train Acc':<15} {'Test Acc':<15} {'Train Match':<15} {'Test Match':<15}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for result in all_results:\n",
    "    task_id = result['task_id']\n",
    "    train_acc = result.get('train_accuracy', 0.0)\n",
    "    test_acc = result.get('test_accuracy', 0.0)\n",
    "    train_match = result.get('train_exact_matches', 0)\n",
    "    test_match = result.get('test_exact_matches', 0)\n",
    "    train_total = len(result.get('train_examples', []))\n",
    "    test_total = len(result.get('test_examples', []))\n",
    "    \n",
    "    total_train_accuracy.append(train_acc)\n",
    "    total_test_accuracy.append(test_acc)\n",
    "    total_train_exact += train_match\n",
    "    total_test_exact += test_match\n",
    "    total_train_examples += train_total\n",
    "    total_test_examples += test_total\n",
    "    \n",
    "    print(f\"{task_id:<15} {train_acc*100:>6.1f}%{'':<8} {test_acc*100:>6.1f}%{'':<8} \"\n",
    "          f\"{train_match}/{train_total}{'':<10} {test_match}/{test_total}{'':<10}\")\n",
    "\n",
    "print(\"-\" * 70)\n",
    "print(f\"\\n{'OVERALL':<15} {np.mean(total_train_accuracy)*100:>6.1f}%{'':<8} \"\n",
    "      f\"{np.mean(total_test_accuracy)*100:>6.1f}%{'':<8} \"\n",
    "      f\"{total_train_exact}/{total_train_examples}{'':<10} \"\n",
    "      f\"{total_test_exact}/{total_test_examples}{'':<10}\")\n",
    "\n",
    "print(f\"\\n> Summary:\")\n",
    "print(f\"   • Average Training Accuracy: {np.mean(total_train_accuracy)*100:.1f}%\")\n",
    "print(f\"   • Average Test Accuracy: {np.mean(total_test_accuracy)*100:.1f}%\")\n",
    "print(f\"   • Total Training Exact Matches: {total_train_exact}/{total_train_examples} \"\n",
    "      f\"({total_train_exact/total_train_examples*100:.1f}%)\")\n",
    "print(f\"   • Total Test Exact Matches: {total_test_exact}/{total_test_examples} \"\n",
    "      f\"({total_test_exact/total_test_examples*100:.1f}%)\")\n",
    "print(f\"   • Tasks Tested: {len(all_results)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2188fd32",
   "metadata": {},
   "source": [
    "## Analysis and Insights\n",
    "\n",
    "Key findings from the testing process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad411a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task-by-Task Analysis:\n",
      "\n",
      "\n",
      ">Task 05f2a901:\n",
      "OK: All training examples solved correctly!\n",
      "OK: All test examples solved correctly!\n",
      "\n",
      ">Task 1cf80156:\n",
      "OK: All training examples solved correctly!\n",
      "OK: All test examples solved correctly!\n",
      "\n",
      ">Task 1e0a9b12:\n",
      "OK: All training examples solved correctly!\n",
      "OK: All test examples solved correctly!\n",
      "\n",
      ">Task 2bcee788:\n",
      "OK: All training examples solved correctly!\n",
      "OK: All test examples solved correctly!\n",
      "\n",
      ">Task 7ddcd7ec:\n",
      "OK: All training examples solved correctly!\n",
      "OK: All test examples solved correctly!\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"Task-by-Task Analysis:\\n\")\n",
    "\n",
    "for result in all_results:\n",
    "    task_id = result['task_id']\n",
    "    print(f\"\\n>Task {task_id}:\")\n",
    "    \n",
    "    if 'error' in result and 'train_examples' not in result:\n",
    "        print(f\">>>>Critical Error: {result['error']}\")\n",
    "        continue\n",
    "    \n",
    "    # Check if all training examples passed\n",
    "    train_examples = result.get('train_examples', [])\n",
    "    test_examples = result.get('test_examples', [])\n",
    "    \n",
    "    if train_examples:\n",
    "        all_train_pass = all(ex.get('is_exact', False) for ex in train_examples)\n",
    "        if all_train_pass:\n",
    "            print(f\"OK: All training examples solved correctly!\")\n",
    "        else:\n",
    "            failed = [ex['example_idx']+1 for ex in train_examples if not ex.get('is_exact', False)]\n",
    "            print(f\"Training examples failed: {failed}\")\n",
    "    \n",
    "    if test_examples:\n",
    "        all_test_pass = all(ex.get('is_exact', False) for ex in test_examples)\n",
    "        if all_test_pass:\n",
    "            print(f\"OK: All test examples solved correctly!\")\n",
    "        else:\n",
    "            failed = [ex['example_idx']+1 for ex in test_examples if not ex.get('is_exact', False)]\n",
    "            print(f\"Test examples failed: {failed}\")\n",
    "    \n",
    "    # Check for errors\n",
    "    errors = [ex for ex in train_examples + test_examples if 'error' in ex]\n",
    "    if errors:\n",
    "        print(f\"Encountered {len(errors)} error(s) during execution\")\n",
    "        for err in errors[:2]:  # Show first 2 errors\n",
    "            print(f\"      - {err.get('error', 'Unknown error')[:80]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
